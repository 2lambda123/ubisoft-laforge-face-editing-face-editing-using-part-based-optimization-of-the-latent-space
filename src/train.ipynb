{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import context\n",
        "from models import NEURAL_FACE\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "FORCE_CPU = False\n",
        "import os\n",
        "if FORCE_CPU:\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = \"TRUE\"\n",
        "\n",
        "from core import utils_dataloader\n",
        "from core import face_dataset\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import torch\n",
        "from psbody.mesh import Mesh\n",
        "import pickle as pk\n",
        "from utils import utils, writer, train_eval, mesh_sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cpu' if FORCE_CPU else 'cuda', 0)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Paths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root = rf'{os.path.abspath(os.path.dirname(__file__))}/..'\n",
        "data_path = rf'{root}/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_dir = rf'{data_path}/out_face_model'\n",
        "logs_dir = out_dir + '/logs'\n",
        "checkpoints_dir = out_dir + '/checkpoints'\n",
        "parts_transforms_dir = out_dir + '/parts_transforms'\n",
        "utils.makedirs(out_dir)\n",
        "utils.makedirs(logs_dir)\n",
        "utils.makedirs(checkpoints_dir)\n",
        "utils.makedirs(parts_transforms_dir)\n",
        "writer = writer.Writer(checkpoints_dir, logs_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Dataset configs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_MESH_COUNT = 150\n",
        "TRAIN_VALID_SPLIT = 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Building the mesh cache for faster load times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh_cache_path = rf'{out_dir}/mesh_cache.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(mesh_cache_path):\n",
        "    train_data, test_data, scaler = utils_dataloader.load_face_and_parts(data_path, TRAIN_MESH_COUNT)\n",
        "    pk.dump((train_data, test_data, scaler), open(mesh_cache_path, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data, test_data, scaler = pk.load(open(mesh_cache_path, 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(train_data))\n",
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = face_dataset.Dataset(train_data)\n",
        "test_dataset = face_dataset.Dataset(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Save the scaler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pk.dump(scaler, open(rf'{out_dir}/face_scaler.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Load the parts info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "part_verts = []\n",
        "parts_names = []\n",
        "parts_template = []\n",
        "vert_map = np.loadtxt(rf'{data_path}/vert_map.csv', dtype=np.uint32)\n",
        "\n",
        "for path in sorted(glob(rf'{data_path}/parts_info/*')):\n",
        "    verts = np.loadtxt(path, dtype=np.uint32)\n",
        "    for idx, vert in enumerate(verts):\n",
        "        verts[idx] = vert_map[vert]\n",
        "    part_verts.append(verts.tolist())\n",
        "    part_name = os.path.basename(os.path.normpath(path)).split('.')[0]\n",
        "    parts_names.append(part_name)\n",
        "    parts_template.append(rf'{data_path}/mesh_data/parts/{part_name}_1.obj')\n",
        "    \n",
        "\n",
        "parts_num = len(parts_names)\n",
        "print(parts_num)\n",
        "print(parts_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Creating the down/up sampling matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_factors = [2, 2]\n",
        "down_transform_list = []\n",
        "down_edge_index_list = []\n",
        "\n",
        "for part_name, template_fp in zip(parts_names, parts_template):\n",
        "    transform_fp = rf'{parts_transforms_dir}/{part_name}.pkl'\n",
        "    if not osp.exists(transform_fp):\n",
        "        print('Generating transform matrices...')\n",
        "        mesh = Mesh(filename=template_fp)\n",
        "        \n",
        "        _, A, D, U, F = mesh_sampling.generate_transform_matrices(mesh, ds_factors)\n",
        "        tmp = {'face': F, 'adj': A, 'down_transform': D, 'up_transform': U}\n",
        "\n",
        "        pk.dump(tmp, open(transform_fp, 'wb'))\n",
        "        print('Done!')\n",
        "        print('Transform matrices are saved in \\'{}\\''.format(transform_fp))\n",
        "    else:\n",
        "        tmp = pk.load(open(transform_fp, 'rb'), encoding='latin1')\n",
        "\n",
        "    edge_index = [utils.to_edge_index(adj).to(device) for adj in tmp['adj']]\n",
        "    down_transforms = [\n",
        "        utils.to_sparse(down_transform).to(device)\n",
        "        for down_transform in tmp['down_transform']\n",
        "    ]\n",
        "\n",
        "    down_edge_index_list.append(edge_index)\n",
        "    down_transform_list.append(down_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform_fp = out_dir + '/face_transform.pkl'\n",
        "template_fp = rf'{data_path}/mesh_data/faces/face (1).obj'\n",
        "\n",
        "if not osp.exists(transform_fp):\n",
        "    print('Generating transform matrices...')\n",
        "    mesh = Mesh(filename=template_fp)\n",
        "\n",
        "    _, A, D, U, F = mesh_sampling.generate_transform_matrices(mesh, ds_factors)\n",
        "    tmp = {'face': F, 'adj': A, 'down_transform': D, 'up_transform': U}\n",
        "\n",
        "    pk.dump(tmp, open(transform_fp, 'wb'))\n",
        "    print('Done!')\n",
        "    print('Transform matrices are saved in \\'{}\\''.format(transform_fp))\n",
        "else:\n",
        "    tmp = pk.load(open(transform_fp, 'rb'), encoding='latin1')\n",
        "\n",
        "up_edge_index = [utils.to_edge_index(adj).to(device) for adj in tmp['adj']]\n",
        "up_transforms = [\n",
        "    utils.to_sparse(up_transform).to(device)\n",
        "    for up_transform in tmp['up_transform']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "K = 6\n",
        "in_channels = 3\n",
        "part_latent_size = 8\n",
        "out_channels = [16, 32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 73\n",
        "\n",
        "lr = 8e-4\n",
        "lr_decay = 0.99\n",
        "\n",
        "decay_step = 1\n",
        "weight_decay = 0\n",
        "beta = 1e-3  # 0.0055\n",
        "ceta = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = NEURAL_FACE(in_channels,\n",
        "                    out_channels,\n",
        "                    part_latent_size,\n",
        "                    down_edge_index_list,\n",
        "                    down_transform_list,\n",
        "                    up_edge_index,\n",
        "                    up_transforms,\n",
        "                    K=K).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, decay_step, gamma=lr_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### For faster training, we don't save the model while training, and stop at epoch 73, which produces the best result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_eval.run(model, train_loader, epochs, optimizer, scheduler, writer, device, part_verts,\n",
        "                     part_latent_size, beta=beta, ceta=ceta, save_all=True, dont_save=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "writer.save_checkpoint(model, optimizer, scheduler, 73)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "PCA",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a33a2c4d0630d5b42d29029b32b8dc5d59183b16071ae3e7a4b534a0cff5a5b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
